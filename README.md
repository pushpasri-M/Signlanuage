# Project Title: Real-Time Sign Language Recognition System Using Deep Learning and Sensor Fusion
# ğŸ§  Sign Language Recognition System

A real-time Sign Language Recognition System using deep learning and sensor fusion with MediaPipe, Flex Sensors, and IMU (MPU6050). This project helps bridge the communication gap between hearing-impaired individuals and others by converting hand gestures into spoken words and text.

---

## ğŸ“Œ Features

- ğŸ”´ Real-time gesture recognition
- ğŸ§© Sensor fusion: Flex sensors + MPU6050 + MediaPipe hand landmarks
- ğŸ“Š Hybrid deep learning model (1D-CNN + BiLSTM)
- ğŸ—£ï¸ Voice output using `pyttsx3`
- ğŸ–¥ï¸ Live webcam feed and gesture display
- ğŸ§  Trained on custom dataset using PyTorch
- ğŸ§ª Accurate recognition of dynamic and static gestures

---

## ğŸ› ï¸ Technologies Used

| Category              | Tools/Technologies                             |
|-----------------------|------------------------------------------------|
| Programming Language  | Python, Arduino C                              |
| Libraries/Frameworks  | PyTorch, OpenCV, MediaPipe, pyttsx3, NumPy     |
| Microcontrollers      | ESP32 (with Arduino IDE)                       |
| Sensors               | Flex Sensor, MPU6050 IMU                       |
| ML Algorithms         | 1D-CNN, BiLSTM, RandomForest (initial version)|
| Data Handling         | pandas, joblib, scikit-learn                   |

---

## ğŸ”§ Setup Instructions

### 1. Hardware Connections

- Connect **Flex Sensors** to analog pins of ESP32.  
- Connect **MPU6050 (IMU)** via I2C (SCL/SDA) to ESP32.  
- Ensure ESP32 sends serial data in a structured format (e.g., comma-separated).

### 2. Software Installation


# Clone the repo
git clone https://github.com/yourusername/sign-language-recognition.git
cd sign-language-recognition

# Install Python dependencies
pip install -r requirements.txt

# Data Flow Overview
Sensors collect finger bend and wrist movement data.

MediaPipe captures 3D hand landmarks.

Combined data is processed through a hybrid model (1D-CNN + BiLSTM).

Predicted gesture is translated into text and spoken via pyttsx3.



## ğŸš€ Project Structure

```bash
sign-language-recognition/
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ gesture_model.pkl         # Trained RandomForest or CNN+BiLSTM model
â”‚   â”œâ”€â”€ scaler.pkl                # StandardScaler for input normalization
â”‚   â”œâ”€â”€ label_encoder.pkl         # For decoding predicted classes
â”‚   â””â”€â”€ feature_names.txt         # Ordered feature names for prediction
â”‚
â”œâ”€â”€ arduino/
â”‚   â””â”€â”€ flex_imu_sender.ino       # Arduino code for sending sensor data
â”‚
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ *.csv                     # Labeled sensor + MediaPipe data
â”‚
â”œâ”€â”€ main.py                       # Main Python script for real-time recognition
â”œâ”€â”€ train_model.py                # Script to train the ML/DL model
â”œâ”€â”€ requirements.txt              # All dependencies
â””â”€â”€ README.md                     # Project documentation


